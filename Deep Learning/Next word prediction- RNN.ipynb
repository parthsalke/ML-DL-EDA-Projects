{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction using RNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"C:\\\\Users\\\\Parth Salke\\\\Downloads\\\\Datasets\\\\metamorphosis-clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729],\n",
       "       [729, 135],\n",
       "       [135, 730],\n",
       "       [730, 294],\n",
       "       [294,   8],\n",
       "       [  8, 731]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating RNN/LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             26170     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2617)              2619617   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,694,787\n",
      "Trainable params: 15,694,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8753\n",
      "Epoch 1: loss improved from inf to 7.87530, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 141ms/step - loss: 7.8753 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8629\n",
      "Epoch 2: loss improved from 7.87530 to 7.86287, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 7.8629 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8268\n",
      "Epoch 3: loss improved from 7.86287 to 7.82678, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 7.8268 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.6802\n",
      "Epoch 4: loss improved from 7.82678 to 7.68016, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 178ms/step - loss: 7.6802 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.4860\n",
      "Epoch 5: loss improved from 7.68016 to 7.48601, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 7.4860 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.3053\n",
      "Epoch 6: loss improved from 7.48601 to 7.30528, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 7.3053 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.1484\n",
      "Epoch 7: loss improved from 7.30528 to 7.14841, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 172ms/step - loss: 7.1484 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.9556\n",
      "Epoch 8: loss improved from 7.14841 to 6.95563, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 6.9556 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.7118\n",
      "Epoch 9: loss improved from 6.95563 to 6.71176, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 6.7118 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.4671\n",
      "Epoch 10: loss improved from 6.71176 to 6.46713, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 164ms/step - loss: 6.4671 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.2479\n",
      "Epoch 11: loss improved from 6.46713 to 6.24786, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 6.2479 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.0556\n",
      "Epoch 12: loss improved from 6.24786 to 6.05560, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 6.0556 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.8644\n",
      "Epoch 13: loss improved from 6.05560 to 5.86438, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 5.8644 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.6896\n",
      "Epoch 14: loss improved from 5.86438 to 5.68956, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 5.6896 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.5228\n",
      "Epoch 15: loss improved from 5.68956 to 5.52275, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 5.5228 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.3838\n",
      "Epoch 16: loss improved from 5.52275 to 5.38382, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 5.3838 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.2594\n",
      "Epoch 17: loss improved from 5.38382 to 5.25943, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 5.2594 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.1257\n",
      "Epoch 18: loss improved from 5.25943 to 5.12566, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 5.1257 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.0037\n",
      "Epoch 19: loss improved from 5.12566 to 5.00373, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 178ms/step - loss: 5.0037 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.9321\n",
      "Epoch 20: loss improved from 5.00373 to 4.93206, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 4.9321 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.8302\n",
      "Epoch 21: loss improved from 4.93206 to 4.83017, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 175ms/step - loss: 4.8302 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.7837\n",
      "Epoch 22: loss improved from 4.83017 to 4.78369, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 182ms/step - loss: 4.7837 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.6846\n",
      "Epoch 23: loss improved from 4.78369 to 4.68461, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 179ms/step - loss: 4.6846 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.6028\n",
      "Epoch 24: loss improved from 4.68461 to 4.60277, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 172ms/step - loss: 4.6028 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.5396\n",
      "Epoch 25: loss improved from 4.60277 to 4.53961, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 4.5396 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.4452\n",
      "Epoch 26: loss improved from 4.53961 to 4.44520, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 4.4452 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.3782\n",
      "Epoch 27: loss improved from 4.44520 to 4.37822, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 4.3782 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.2610\n",
      "Epoch 28: loss improved from 4.37822 to 4.26102, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 4.2610 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.1116\n",
      "Epoch 29: loss improved from 4.26102 to 4.11157, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 4.1116 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.9755\n",
      "Epoch 30: loss improved from 4.11157 to 3.97546, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 3.9755 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.8462\n",
      "Epoch 31: loss improved from 3.97546 to 3.84617, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 3.8462 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.7108\n",
      "Epoch 32: loss improved from 3.84617 to 3.71084, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 3.7108 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.5571\n",
      "Epoch 33: loss improved from 3.71084 to 3.55713, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 3.5571 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.4590\n",
      "Epoch 34: loss improved from 3.55713 to 3.45896, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 3.4590 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.3316\n",
      "Epoch 35: loss improved from 3.45896 to 3.33161, saving model to nextword1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 9s 144ms/step - loss: 3.3316 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.2204\n",
      "Epoch 36: loss improved from 3.33161 to 3.22039, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 3.2204 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.1109\n",
      "Epoch 37: loss improved from 3.22039 to 3.11089, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 3.1109 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0691\n",
      "Epoch 38: loss improved from 3.11089 to 3.06914, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 3.0691 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.9922\n",
      "Epoch 39: loss improved from 3.06914 to 2.99223, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 178ms/step - loss: 2.9922 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.8791\n",
      "Epoch 40: loss improved from 2.99223 to 2.87915, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 2.8791 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.8195\n",
      "Epoch 41: loss improved from 2.87915 to 2.81954, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 2.8195 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.7297\n",
      "Epoch 42: loss improved from 2.81954 to 2.72969, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 2.7297 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.6711\n",
      "Epoch 43: loss improved from 2.72969 to 2.67107, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 2.6711 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.6384\n",
      "Epoch 44: loss improved from 2.67107 to 2.63837, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 2.6384 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.5496\n",
      "Epoch 45: loss improved from 2.63837 to 2.54965, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 2.5496 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.4731\n",
      "Epoch 46: loss improved from 2.54965 to 2.47312, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 2.4731 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.4477\n",
      "Epoch 47: loss improved from 2.47312 to 2.44771, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 2.4477 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3924\n",
      "Epoch 48: loss improved from 2.44771 to 2.39244, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 2.3924 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3757\n",
      "Epoch 49: loss improved from 2.39244 to 2.37572, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 2.3757 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3218\n",
      "Epoch 50: loss improved from 2.37572 to 2.32175, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 2.3218 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2478\n",
      "Epoch 51: loss improved from 2.32175 to 2.24783, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 2.2478 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2091\n",
      "Epoch 52: loss improved from 2.24783 to 2.20911, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 2.2091 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1982\n",
      "Epoch 53: loss improved from 2.20911 to 2.19819, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 2.1982 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1418\n",
      "Epoch 54: loss improved from 2.19819 to 2.14182, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 2.1418 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1094\n",
      "Epoch 55: loss improved from 2.14182 to 2.10939, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 2.1094 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0952\n",
      "Epoch 56: loss improved from 2.10939 to 2.09523, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 2.0952 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0653\n",
      "Epoch 57: loss improved from 2.09523 to 2.06532, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 2.0653 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0520\n",
      "Epoch 58: loss improved from 2.06532 to 2.05204, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 156ms/step - loss: 2.0520 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0227\n",
      "Epoch 59: loss improved from 2.05204 to 2.02274, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 2.0227 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9838\n",
      "Epoch 60: loss improved from 2.02274 to 1.98377, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 1.9838 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9357\n",
      "Epoch 61: loss improved from 1.98377 to 1.93572, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 1.9357 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8846\n",
      "Epoch 62: loss improved from 1.93572 to 1.88457, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 1.8846 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8793\n",
      "Epoch 63: loss improved from 1.88457 to 1.87928, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 162ms/step - loss: 1.8793 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8481\n",
      "Epoch 64: loss improved from 1.87928 to 1.84807, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 185ms/step - loss: 1.8481 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8230\n",
      "Epoch 65: loss improved from 1.84807 to 1.82301, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 1.8230 - lr: 0.0010\n",
      "Epoch 66/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7781\n",
      "Epoch 66: loss improved from 1.82301 to 1.77809, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 1.7781 - lr: 0.0010\n",
      "Epoch 67/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7662\n",
      "Epoch 67: loss improved from 1.77809 to 1.76618, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 1.7662 - lr: 0.0010\n",
      "Epoch 68/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7583\n",
      "Epoch 68: loss improved from 1.76618 to 1.75832, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 1.7583 - lr: 0.0010\n",
      "Epoch 69/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7364\n",
      "Epoch 69: loss improved from 1.75832 to 1.73643, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 1.7364 - lr: 0.0010\n",
      "Epoch 70/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - ETA: 0s - loss: 1.6951\n",
      "Epoch 70: loss improved from 1.73643 to 1.69514, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 1.6951 - lr: 0.0010\n",
      "Epoch 71/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6773\n",
      "Epoch 71: loss improved from 1.69514 to 1.67729, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 1.6773 - lr: 0.0010\n",
      "Epoch 72/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7006\n",
      "Epoch 72: loss did not improve from 1.67729\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 1.7006 - lr: 0.0010\n",
      "Epoch 73/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6729\n",
      "Epoch 73: loss improved from 1.67729 to 1.67285, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 1.6729 - lr: 0.0010\n",
      "Epoch 74/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6347\n",
      "Epoch 74: loss improved from 1.67285 to 1.63470, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 1.6347 - lr: 0.0010\n",
      "Epoch 75/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6105\n",
      "Epoch 75: loss improved from 1.63470 to 1.61047, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 1.6105 - lr: 0.0010\n",
      "Epoch 76/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5938\n",
      "Epoch 76: loss improved from 1.61047 to 1.59378, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 1.5938 - lr: 0.0010\n",
      "Epoch 77/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5684\n",
      "Epoch 77: loss improved from 1.59378 to 1.56837, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 1.5684 - lr: 0.0010\n",
      "Epoch 78/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5556\n",
      "Epoch 78: loss improved from 1.56837 to 1.55563, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 1.5556 - lr: 0.0010\n",
      "Epoch 79/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5620\n",
      "Epoch 79: loss did not improve from 1.55563\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 1.5620 - lr: 0.0010\n",
      "Epoch 80/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5329\n",
      "Epoch 80: loss improved from 1.55563 to 1.53295, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 171ms/step - loss: 1.5329 - lr: 0.0010\n",
      "Epoch 81/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5128\n",
      "Epoch 81: loss improved from 1.53295 to 1.51282, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 182ms/step - loss: 1.5128 - lr: 0.0010\n",
      "Epoch 82/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4965\n",
      "Epoch 82: loss improved from 1.51282 to 1.49651, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 169ms/step - loss: 1.4965 - lr: 0.0010\n",
      "Epoch 83/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4997\n",
      "Epoch 83: loss did not improve from 1.49651\n",
      "61/61 [==============================] - 9s 139ms/step - loss: 1.4997 - lr: 0.0010\n",
      "Epoch 84/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4972\n",
      "Epoch 84: loss did not improve from 1.49651\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 1.4972 - lr: 0.0010\n",
      "Epoch 85/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4611\n",
      "Epoch 85: loss improved from 1.49651 to 1.46113, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 1.4611 - lr: 0.0010\n",
      "Epoch 86/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 86: loss improved from 1.46113 to 1.42799, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 1.4280 - lr: 0.0010\n",
      "Epoch 87/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 87: loss improved from 1.42799 to 1.41547, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 1.4155 - lr: 0.0010\n",
      "Epoch 88/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 88: loss did not improve from 1.41547\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 1.4360 - lr: 0.0010\n",
      "Epoch 89/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 89: loss did not improve from 1.41547\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 1.4502 - lr: 0.0010\n",
      "Epoch 90/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 90: loss improved from 1.41547 to 1.41484, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 1.4148 - lr: 0.0010\n",
      "Epoch 91/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3823\n",
      "Epoch 91: loss improved from 1.41484 to 1.38229, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 1.3823 - lr: 0.0010\n",
      "Epoch 92/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3618\n",
      "Epoch 92: loss improved from 1.38229 to 1.36176, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 1.3618 - lr: 0.0010\n",
      "Epoch 93/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3334\n",
      "Epoch 93: loss improved from 1.36176 to 1.33336, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 1.3334 - lr: 0.0010\n",
      "Epoch 94/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3081\n",
      "Epoch 94: loss improved from 1.33336 to 1.30810, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 1.3081 - lr: 0.0010\n",
      "Epoch 95/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2986\n",
      "Epoch 95: loss improved from 1.30810 to 1.29861, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 1.2986 - lr: 0.0010\n",
      "Epoch 96/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2860\n",
      "Epoch 96: loss improved from 1.29861 to 1.28601, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 1.2860 - lr: 0.0010\n",
      "Epoch 97/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2769\n",
      "Epoch 97: loss improved from 1.28601 to 1.27694, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 1.2769 - lr: 0.0010\n",
      "Epoch 98/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2564\n",
      "Epoch 98: loss improved from 1.27694 to 1.25641, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 1.2564 - lr: 0.0010\n",
      "Epoch 99/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2561\n",
      "Epoch 99: loss improved from 1.25641 to 1.25608, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 183ms/step - loss: 1.2561 - lr: 0.0010\n",
      "Epoch 100/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2214\n",
      "Epoch 100: loss improved from 1.25608 to 1.22136, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 1.2214 - lr: 0.0010\n",
      "Epoch 101/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2089\n",
      "Epoch 101: loss improved from 1.22136 to 1.20889, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 1.2089 - lr: 0.0010\n",
      "Epoch 102/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1930\n",
      "Epoch 102: loss improved from 1.20889 to 1.19302, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 1.1930 - lr: 0.0010\n",
      "Epoch 103/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1903\n",
      "Epoch 103: loss improved from 1.19302 to 1.19033, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 1.1903 - lr: 0.0010\n",
      "Epoch 104/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1918\n",
      "Epoch 104: loss did not improve from 1.19033\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 1.1918 - lr: 0.0010\n",
      "Epoch 105/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1614\n",
      "Epoch 105: loss improved from 1.19033 to 1.16136, saving model to nextword1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 9s 142ms/step - loss: 1.1614 - lr: 0.0010\n",
      "Epoch 106/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1444\n",
      "Epoch 106: loss improved from 1.16136 to 1.14440, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 1.1444 - lr: 0.0010\n",
      "Epoch 107/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1266\n",
      "Epoch 107: loss improved from 1.14440 to 1.12664, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 1.1266 - lr: 0.0010\n",
      "Epoch 108/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1242\n",
      "Epoch 108: loss improved from 1.12664 to 1.12423, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 1.1242 - lr: 0.0010\n",
      "Epoch 109/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1154\n",
      "Epoch 109: loss improved from 1.12423 to 1.11535, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 1.1154 - lr: 0.0010\n",
      "Epoch 110/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1313\n",
      "Epoch 110: loss did not improve from 1.11535\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 1.1313 - lr: 0.0010\n",
      "Epoch 111/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1145\n",
      "Epoch 111: loss improved from 1.11535 to 1.11453, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 1.1145 - lr: 0.0010\n",
      "Epoch 112/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0977\n",
      "Epoch 112: loss improved from 1.11453 to 1.09772, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 1.0977 - lr: 0.0010\n",
      "Epoch 113/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1027\n",
      "Epoch 113: loss did not improve from 1.09772\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 1.1027 - lr: 0.0010\n",
      "Epoch 114/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0939\n",
      "Epoch 114: loss improved from 1.09772 to 1.09393, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 1.0939 - lr: 0.0010\n",
      "Epoch 115/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0738\n",
      "Epoch 115: loss improved from 1.09393 to 1.07381, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 1.0738 - lr: 0.0010\n",
      "Epoch 116/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0781\n",
      "Epoch 116: loss did not improve from 1.07381\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 1.0781 - lr: 0.0010\n",
      "Epoch 117/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0804\n",
      "Epoch 117: loss did not improve from 1.07381\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 1.0804 - lr: 0.0010\n",
      "Epoch 118/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0925\n",
      "Epoch 118: loss did not improve from 1.07381\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 1.0925 - lr: 0.0010\n",
      "Epoch 119/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8320\n",
      "Epoch 119: loss improved from 1.07381 to 0.83200, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.8320 - lr: 2.0000e-04\n",
      "Epoch 120/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7445\n",
      "Epoch 120: loss improved from 0.83200 to 0.74455, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 156ms/step - loss: 0.7445 - lr: 2.0000e-04\n",
      "Epoch 121/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7175\n",
      "Epoch 121: loss improved from 0.74455 to 0.71752, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.7175 - lr: 2.0000e-04\n",
      "Epoch 122/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7064\n",
      "Epoch 122: loss improved from 0.71752 to 0.70642, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.7064 - lr: 2.0000e-04\n",
      "Epoch 123/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6984\n",
      "Epoch 123: loss improved from 0.70642 to 0.69839, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.6984 - lr: 2.0000e-04\n",
      "Epoch 124/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6954\n",
      "Epoch 124: loss improved from 0.69839 to 0.69535, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.6954 - lr: 2.0000e-04\n",
      "Epoch 125/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6917\n",
      "Epoch 125: loss improved from 0.69535 to 0.69170, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 0.6917 - lr: 2.0000e-04\n",
      "Epoch 126/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6918\n",
      "Epoch 126: loss did not improve from 0.69170\n",
      "61/61 [==============================] - 10s 171ms/step - loss: 0.6918 - lr: 2.0000e-04\n",
      "Epoch 127/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6874\n",
      "Epoch 127: loss improved from 0.69170 to 0.68741, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 0.6874 - lr: 2.0000e-04\n",
      "Epoch 128/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6866\n",
      "Epoch 128: loss improved from 0.68741 to 0.68662, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.6866 - lr: 2.0000e-04\n",
      "Epoch 129/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6856\n",
      "Epoch 129: loss improved from 0.68662 to 0.68559, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 0.6856 - lr: 2.0000e-04\n",
      "Epoch 130/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6863\n",
      "Epoch 130: loss did not improve from 0.68559\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 0.6863 - lr: 2.0000e-04\n",
      "Epoch 131/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6860\n",
      "Epoch 131: loss did not improve from 0.68559\n",
      "61/61 [==============================] - 11s 188ms/step - loss: 0.6860 - lr: 2.0000e-04\n",
      "Epoch 132/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6839\n",
      "Epoch 132: loss improved from 0.68559 to 0.68393, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 0.6839 - lr: 2.0000e-04\n",
      "Epoch 133/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6854\n",
      "Epoch 133: loss did not improve from 0.68393\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.6854 - lr: 2.0000e-04\n",
      "Epoch 134/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6850\n",
      "Epoch 134: loss did not improve from 0.68393\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.6850 - lr: 2.0000e-04\n",
      "Epoch 135/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6853\n",
      "Epoch 135: loss did not improve from 0.68393\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.6853 - lr: 2.0000e-04\n",
      "Epoch 136/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6430\n",
      "Epoch 136: loss improved from 0.68393 to 0.64296, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.6430 - lr: 1.0000e-04\n",
      "Epoch 137/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6408\n",
      "Epoch 137: loss improved from 0.64296 to 0.64079, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.6408 - lr: 1.0000e-04\n",
      "Epoch 138/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6404\n",
      "Epoch 138: loss improved from 0.64079 to 0.64045, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.6404 - lr: 1.0000e-04\n",
      "Epoch 139/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6389\n",
      "Epoch 139: loss improved from 0.64045 to 0.63887, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 0.6389 - lr: 1.0000e-04\n",
      "Epoch 140/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6386\n",
      "Epoch 140: loss improved from 0.63887 to 0.63860, saving model to nextword1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 9s 153ms/step - loss: 0.6386 - lr: 1.0000e-04\n",
      "Epoch 141/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6384\n",
      "Epoch 141: loss improved from 0.63860 to 0.63835, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.6384 - lr: 1.0000e-04\n",
      "Epoch 142/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6383\n",
      "Epoch 142: loss improved from 0.63835 to 0.63832, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.6383 - lr: 1.0000e-04\n",
      "Epoch 143/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6385\n",
      "Epoch 143: loss did not improve from 0.63832\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.6385 - lr: 1.0000e-04\n",
      "Epoch 144/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6379\n",
      "Epoch 144: loss improved from 0.63832 to 0.63788, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 0.6379 - lr: 1.0000e-04\n",
      "Epoch 145/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6367\n",
      "Epoch 145: loss improved from 0.63788 to 0.63672, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.6367 - lr: 1.0000e-04\n",
      "Epoch 146/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6379\n",
      "Epoch 146: loss did not improve from 0.63672\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.6379 - lr: 1.0000e-04\n",
      "Epoch 147/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6367\n",
      "Epoch 147: loss improved from 0.63672 to 0.63671, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.6367 - lr: 1.0000e-04\n",
      "Epoch 148/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6364\n",
      "Epoch 148: loss improved from 0.63671 to 0.63635, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 149/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6358\n",
      "Epoch 149: loss improved from 0.63635 to 0.63578, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.6358 - lr: 1.0000e-04\n",
      "Epoch 150/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6358\n",
      "Epoch 150: loss did not improve from 0.63578\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.6358 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27326d95a30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "        In this function we are using the tokenizer and models trained\n",
    "        and we are creating the sequence of the text entered and then\n",
    "        using our model to predict and return the the predicted word.\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(3):\n",
    "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "        sequence = np.array(sequence)\n",
    "        \n",
    "        preds = model.predict_classes(sequence)\n",
    "        #print(preds)\n",
    "        predicted_word = \"\"\n",
    "        \n",
    "        for key, value in tokenizer.word_index.items():\n",
    "            if value == preds:\n",
    "                predicted_word = key\n",
    "                break\n",
    "        \n",
    "        print(predicted_word)\n",
    "        return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: at your service\n",
      "Enter your line: at the dull\n",
      "Enter your line: of textile\n",
      "Enter your line: stop the script\n",
      "Ending The Program.....\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    We are testing our model and we will run the model\n",
    "    until the user decides to stop the script.\n",
    "    While the script is running we try and check if \n",
    "    the prediction can be made on the text. If no\n",
    "    prediction can be made we just continue.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# text1 = \"at the dull\"\n",
    "# text2 = \"collection of textile\"\n",
    "# text3 = \"what a strenuous\"\n",
    "# text4 = \"stop the script\"\n",
    "\n",
    "while(True):\n",
    "\n",
    "    text = input(\"Enter your line: \")\n",
    "    \n",
    "    if text == \"stop the script\":\n",
    "        print(\"Ending The Program.....\")\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            text = text.split(\" \")\n",
    "            text = text[-1]\n",
    "\n",
    "            text = ''.join(text)\n",
    "            Predict_Next_Words(model, tokenizer, text)\n",
    "            \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
